{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Handling Missing Values\n",
    "\n",
    "  * Dropping Columns\n",
    "  * Dropping Rows\n",
    "  * Various Parameter In Dropping functionalities\n",
    "  * Handling Missing values by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Handling Missing Values').getOrCreate() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Nehal-Panchal:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Handling Missing Values</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x19bf03b3910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inferSchema --> Gives True DataType of a column \n",
    "df_pyspark = spark.read.csv('Test3.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Used for checking datatype\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Nehal|  21|         3| 60000|\n",
      "|  Chirag|  20|         2| 53000|\n",
      "|Devanshu|  19|         1| 25000|\n",
      "|   Aryan|  25|         5| 45000|\n",
      "|   Daksh|  24|         4| 35000|\n",
      "|   Dhyey|  30|         9| 42000|\n",
      "|  Mahesh|NULL|      NULL| 38000|\n",
      "|    NULL|  34|        10| 23000|\n",
      "|    NULL|  36|      NULL|  NULL|\n",
      "|    NULL|NULL|      NULL|  NULL|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Removing the NULL Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **drop(how = 'any',thresh = None, subset = None)** \n",
    "\n",
    "  * This function returns a new DataFrame omitting rows with null values\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "1. how : str, optional\n",
    "\n",
    "            'any' or 'all'\n",
    "\n",
    "            If 'any' , drop a row if it contains any nulls.\n",
    "            If 'all' , drop a row only if all its values are null.\n",
    "\n",
    "2. thresh : int, optional\n",
    "\n",
    "             default None\n",
    "\n",
    "             If Specified, drop rows that have less than 'thresh' non-null values.\n",
    "             This Overwrites the 'how' paramenter.\n",
    "\n",
    "3. subset : str, tuple or list, optional\n",
    "\n",
    "             optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Nehal| 21|         3| 60000|\n",
      "|  Chirag| 20|         2| 53000|\n",
      "|Devanshu| 19|         1| 25000|\n",
      "|   Aryan| 25|         5| 45000|\n",
      "|   Daksh| 24|         4| 35000|\n",
      "|   Dhyey| 30|         9| 42000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Drop or removes all the rows containing NULL values\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Nehal| 21|         3| 60000|\n",
      "|  Chirag| 20|         2| 53000|\n",
      "|Devanshu| 19|         1| 25000|\n",
      "|   Aryan| 25|         5| 45000|\n",
      "|   Daksh| 24|         4| 35000|\n",
      "|   Dhyey| 30|         9| 42000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### how == any\n",
    "\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Nehal|  21|         3| 60000|\n",
      "|  Chirag|  20|         2| 53000|\n",
      "|Devanshu|  19|         1| 25000|\n",
      "|   Aryan|  25|         5| 45000|\n",
      "|   Daksh|  24|         4| 35000|\n",
      "|   Dhyey|  30|         9| 42000|\n",
      "|  Mahesh|NULL|      NULL| 38000|\n",
      "|    NULL|  34|        10| 23000|\n",
      "|    NULL|  36|      NULL|  NULL|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### how == all\n",
    "#Drops only row which have the values are null.\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Nehal| 21|         3| 60000|\n",
      "|  Chirag| 20|         2| 53000|\n",
      "|Devanshu| 19|         1| 25000|\n",
      "|   Aryan| 25|         5| 45000|\n",
      "|   Daksh| 24|         4| 35000|\n",
      "|   Dhyey| 30|         9| 42000|\n",
      "|    NULL| 34|        10| 23000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Threshold --> Non-NULL values\n",
    "#If the row contain the more than or equal to Threshold of no. of non-null values  then it will not remove.\n",
    "#If it less than the  Threshold of no. of non-null values then it will remove the row\n",
    "\n",
    "df_pyspark.na.drop(how='any', thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Nehal| 21|         3| 60000|\n",
      "|  Chirag| 20|         2| 53000|\n",
      "|Devanshu| 19|         1| 25000|\n",
      "|   Aryan| 25|         5| 45000|\n",
      "|   Daksh| 24|         4| 35000|\n",
      "|   Dhyey| 30|         9| 42000|\n",
      "|    NULL| 34|        10| 23000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Subset\n",
    "# Drop NULL values from a specific column\n",
    "\n",
    "df_pyspark.na.drop(how='any', subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filling the Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **fill(value, subset = None)**\n",
    "\n",
    "  * This function Replace null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Nehal|  21|         3| 60000|\n",
      "|        Chirag|  20|         2| 53000|\n",
      "|      Devanshu|  19|         1| 25000|\n",
      "|         Aryan|  25|         5| 45000|\n",
      "|         Daksh|  24|         4| 35000|\n",
      "|         Dhyey|  30|         9| 42000|\n",
      "|        Mahesh|NULL|      NULL| 38000|\n",
      "|Missing Values|  34|        10| 23000|\n",
      "|Missing Values|  36|      NULL|  NULL|\n",
      "|Missing Values|NULL|      NULL|  NULL|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill the missing values according to the data type of coulmn and the given value.\n",
    "\n",
    "# 'Missing Values' --> String\n",
    "df_pyspark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Nehal| 21|         3| 60000|\n",
      "|  Chirag| 20|         2| 53000|\n",
      "|Devanshu| 19|         1| 25000|\n",
      "|   Aryan| 25|         5| 45000|\n",
      "|   Daksh| 24|         4| 35000|\n",
      "|   Dhyey| 30|         9| 42000|\n",
      "|  Mahesh|  0|         0| 38000|\n",
      "|    NULL| 34|        10| 23000|\n",
      "|    NULL| 36|         0|     0|\n",
      "|    NULL|  0|         0|     0|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0 --> Integer\n",
    "df_pyspark.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Nehal|  21|         3| 60000|\n",
      "|        Chirag|  20|         2| 53000|\n",
      "|      Devanshu|  19|         1| 25000|\n",
      "|         Aryan|  25|         5| 45000|\n",
      "|         Daksh|  24|         4| 35000|\n",
      "|         Dhyey|  30|         9| 42000|\n",
      "|        Mahesh|NULL|      NULL| 38000|\n",
      "|Missing Values|  34|        10| 23000|\n",
      "|Missing Values|  36|      NULL|  NULL|\n",
      "|Missing Values|NULL|      NULL|  NULL|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Values',['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Nehal| 21|         3| 60000|\n",
      "|  Chirag| 20|         2| 53000|\n",
      "|Devanshu| 19|         1| 25000|\n",
      "|   Aryan| 25|         5| 45000|\n",
      "|   Daksh| 24|         4| 35000|\n",
      "|   Dhyey| 30|         9| 42000|\n",
      "|  Mahesh|  0|         0| 38000|\n",
      "|    NULL| 34|        10| 23000|\n",
      "|    NULL| 36|         0|  NULL|\n",
      "|    NULL|  0|         0|  NULL|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(0,['age','Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Replacing Missing values by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mean = Imputer(\n",
    "    inputCols=['age','Experience','Salary'], # type: ignore\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']] # type: ignore\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Nehal|  21|         3| 60000|         21|                 3|         60000|\n",
      "|  Chirag|  20|         2| 53000|         20|                 2|         53000|\n",
      "|Devanshu|  19|         1| 25000|         19|                 1|         25000|\n",
      "|   Aryan|  25|         5| 45000|         25|                 5|         45000|\n",
      "|   Daksh|  24|         4| 35000|         24|                 4|         35000|\n",
      "|   Dhyey|  30|         9| 42000|         30|                 9|         42000|\n",
      "|  Mahesh|NULL|      NULL| 38000|         26|                 4|         38000|\n",
      "|    NULL|  34|        10| 23000|         34|                10|         23000|\n",
      "|    NULL|  36|      NULL|  NULL|         36|                 4|         40125|\n",
      "|    NULL|NULL|      NULL|  NULL|         26|                 4|         40125|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer_mean.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_median = Imputer(\n",
    "    inputCols=['age','Experience','Salary'], # type: ignore\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']] # type: ignore\n",
    ").setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Nehal|  21|         3| 60000|         21|                 3|         60000|\n",
      "|  Chirag|  20|         2| 53000|         20|                 2|         53000|\n",
      "|Devanshu|  19|         1| 25000|         19|                 1|         25000|\n",
      "|   Aryan|  25|         5| 45000|         25|                 5|         45000|\n",
      "|   Daksh|  24|         4| 35000|         24|                 4|         35000|\n",
      "|   Dhyey|  30|         9| 42000|         30|                 9|         42000|\n",
      "|  Mahesh|NULL|      NULL| 38000|         24|                 4|         38000|\n",
      "|    NULL|  34|        10| 23000|         34|                10|         23000|\n",
      "|    NULL|  36|      NULL|  NULL|         36|                 4|         38000|\n",
      "|    NULL|NULL|      NULL|  NULL|         24|                 4|         38000|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer_median.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mode = Imputer(\n",
    "    inputCols=['age','Experience','Salary'], # type: ignore\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']] # type: ignore\n",
    ").setStrategy(\"mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Nehal|  21|         3| 60000|         21|                 3|         60000|\n",
      "|  Chirag|  20|         2| 53000|         20|                 2|         53000|\n",
      "|Devanshu|  19|         1| 25000|         19|                 1|         25000|\n",
      "|   Aryan|  25|         5| 45000|         25|                 5|         45000|\n",
      "|   Daksh|  24|         4| 35000|         24|                 4|         35000|\n",
      "|   Dhyey|  30|         9| 42000|         30|                 9|         42000|\n",
      "|  Mahesh|NULL|      NULL| 38000|         19|                 1|         38000|\n",
      "|    NULL|  34|        10| 23000|         34|                10|         23000|\n",
      "|    NULL|  36|      NULL|  NULL|         36|                 1|         23000|\n",
      "|    NULL|NULL|      NULL|  NULL|         19|                 1|         23000|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer_mode.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
