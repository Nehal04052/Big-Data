{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55b19ddd-919f-483e-bfd0-ef23399adbd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Chance of Admission for Higher Studies\n",
    "#### Predict the chances of admission of a student to a Graduate program based on:\n",
    "\n",
    "1. GRE Scores (290 to 340)\n",
    "2. TOEFL Scores (92 to 120)\n",
    "3. University Rating (1 to 5)\n",
    "4. Statement of Purpose (1 to 5)\n",
    "5. Letter of Recommendation Strength (1 to 5)\n",
    "6. Undergraduate CGPA (6.8 to 9.92)\n",
    "7. Research Experience (0 or 1)\n",
    "8. Chance of Admit (0.34 to 0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543a4a3b-635d-4566-a044-0c95f4c599ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/FileStore/tables/Admission_Chance.csv', name='Admission_Chance.csv', size=12905, modificationTime=1720190058000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Cancer.csv', name='Cancer.csv', size=125204, modificationTime=1720190099000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Credit_Default.csv', name='Credit_Default.csv', size=101152, modificationTime=1720190106000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Customer_Purchase.csv', name='Customer_Purchase.csv', size=1489, modificationTime=1720190113000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Fish.csv', name='Fish.csv', size=6349, modificationTime=1720190119000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Ice_Cream.csv', name='Ice_Cream.csv', size=4872, modificationTime=1720190124000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Test1.csv', name='Test1.csv', size=108, modificationTime=1720158698000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Test2.csv', name='Test2.csv', size=192, modificationTime=1720158698000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Test3.csv', name='Test3.csv', size=388, modificationTime=1720158699000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Test4.csv', name='Test4.csv', size=143, modificationTime=1720158699000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Test5.csv', name='Test5.csv', size=262, modificationTime=1720158699000)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"dbfs:/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4ddb0b-7d86-4e5c-b772-08c3b329c48f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a98b08d-3c47-404e-b861-d8dae44e13d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Chance of Admission for Higher Studies').getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b25f3d2-d952-4f03-b0f3-6493604a6430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3338812497204728#setting/sparkui/0707-033416-dzfopq7d/driver-4352685867671526617\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0d2d60ee00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00bc44f7-a95f-43af-a7fb-841879bf7ced",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_pyspark = spark.read.csv('dbfs:/FileStore/tables/Admission_Chance.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7949da5-993c-4fe6-9714-517e33bb4e29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Serial No: integer (nullable = true)\n |-- GRE Score: integer (nullable = true)\n |-- TOEFL Score: integer (nullable = true)\n |-- University Rating: integer (nullable = true)\n |--  SOP: double (nullable = true)\n |-- LOR : double (nullable = true)\n |-- CGPA: double (nullable = true)\n |-- Research: integer (nullable = true)\n |-- Chance of Admit : double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e95014-dd60-43d3-9300-438e459e5c04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[Serial No: int, GRE Score: int, TOEFL Score: int, University Rating: int,  SOP: double, LOR : double, CGPA: double, Research: int, Chance of Admit : double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b661ea-2574-4805-815c-6f66927bf2b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-----------------+----+----+----+--------+----------------+\n|Serial No|GRE Score|TOEFL Score|University Rating| SOP|LOR |CGPA|Research|Chance of Admit |\n+---------+---------+-----------+-----------------+----+----+----+--------+----------------+\n|        1|      337|        118|                4| 4.5| 4.5|9.65|       1|            0.92|\n|        2|      324|        107|                4| 4.0| 4.5|8.87|       1|            0.76|\n|        3|      316|        104|                3| 3.0| 3.5| 8.0|       1|            0.72|\n|        4|      322|        110|                3| 3.5| 2.5|8.67|       1|             0.8|\n|        5|      314|        103|                2| 2.0| 3.0|8.21|       0|            0.65|\n|        6|      330|        115|                5| 4.5| 3.0|9.34|       1|             0.9|\n|        7|      321|        109|                3| 3.0| 4.0| 8.2|       1|            0.75|\n|        8|      308|        101|                2| 3.0| 4.0| 7.9|       0|            0.68|\n|        9|      302|        102|                1| 2.0| 1.5| 8.0|       0|             0.5|\n|       10|      323|        108|                3| 3.5| 3.0| 8.6|       0|            0.45|\n|       11|      325|        106|                3| 3.5| 4.0| 8.4|       1|            0.52|\n|       12|      327|        111|                4| 4.0| 4.5| 9.0|       1|            0.84|\n|       13|      328|        112|                4| 4.0| 4.5| 9.1|       1|            0.78|\n|       14|      307|        109|                3| 4.0| 3.0| 8.0|       1|            0.62|\n|       15|      311|        104|                3| 3.5| 2.0| 8.2|       1|            0.61|\n|       16|      314|        105|                3| 3.5| 2.5| 8.3|       0|            0.54|\n|       17|      317|        107|                3| 4.0| 3.0| 8.7|       0|            0.66|\n|       18|      319|        106|                3| 4.0| 3.0| 8.0|       1|            0.65|\n|       19|      318|        110|                3| 4.0| 3.0| 8.8|       0|            0.63|\n|       20|      303|        102|                3| 3.5| 3.0| 8.5|       0|            0.62|\n+---------+---------+-----------+-----------------+----+----+----+--------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dd6f99-035f-42ba-91f8-ab06678c725c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Clean the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70737ef8-6ec9-4d0e-b8ce-eb683c2b1786",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Serial_No: integer (nullable = true)\n |-- GRE_Score: integer (nullable = true)\n |-- TOEFL_Score: integer (nullable = true)\n |-- University_Rating: integer (nullable = true)\n |-- SOP: double (nullable = true)\n |-- LOR: double (nullable = true)\n |-- CGPA: double (nullable = true)\n |-- Research: integer (nullable = true)\n |-- Chance_of_Admit: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Clean column names by trimming spaces\n",
    "for col_name in df_pyspark.columns:\n",
    "    df_pyspark = df_pyspark.withColumnRenamed(col_name, col_name.strip().replace(' ', '_'))\n",
    "\n",
    "# Show schema after cleaning\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d415b2-53a4-4cfa-9d3e-98dd410c5e04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing values if necessary\n",
    "df_pyspark = df_pyspark.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce51c04f-8eef-4612-b38e-3ab48e339289",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prepare the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a68c59-f7e9-4a86-8534-6c86f2a6f0c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the feature columns\n",
    "feature_columns = [\"GRE_Score\", \"TOEFL_Score\", \"University_Rating\", \"SOP\", \"LOR\", \"CGPA\", \"Research\"]\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_pyspark = assembler.transform(df_pyspark)\n",
    "\n",
    "# Select only the features and target column\n",
    "df_pyspark = df_pyspark.select(\"features\", \"Chance_of_Admit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c632c9-16a8-4869-9222-2f10602226b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n|            features|Chance_of_Admit|\n+--------------------+---------------+\n|[337.0,118.0,4.0,...|           0.92|\n|[324.0,107.0,4.0,...|           0.76|\n|[316.0,104.0,3.0,...|           0.72|\n|[322.0,110.0,3.0,...|            0.8|\n|[314.0,103.0,2.0,...|           0.65|\n|[330.0,115.0,5.0,...|            0.9|\n|[321.0,109.0,3.0,...|           0.75|\n|[308.0,101.0,2.0,...|           0.68|\n|[302.0,102.0,1.0,...|            0.5|\n|[323.0,108.0,3.0,...|           0.45|\n|[325.0,106.0,3.0,...|           0.52|\n|[327.0,111.0,4.0,...|           0.84|\n|[328.0,112.0,4.0,...|           0.78|\n|[307.0,109.0,3.0,...|           0.62|\n|[311.0,104.0,3.0,...|           0.61|\n|[314.0,105.0,3.0,...|           0.54|\n|[317.0,107.0,3.0,...|           0.66|\n|[319.0,106.0,3.0,...|           0.65|\n|[318.0,110.0,3.0,...|           0.63|\n|[303.0,102.0,3.0,...|           0.62|\n+--------------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c703d579-11fe-4880-bff0-9c49f6481c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Split the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060b678c-04ef-4b90-b70d-9259a2e08099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = df_pyspark.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51be140-521c-4c0a-9e7a-efa35851e39a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca5c740-dea6-4707-8c1a-a17de483e462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Chance_of_Admit\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da33c98b-fcbc-492c-815d-813ee1c48608",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "518a3511-88c5-4743-a7c7-54f1752f01d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0021441557278328955,0.0029448787202701206,0.010158762766901787,-0.007798086707145683,0.021356248341924733,0.11698856317086154,0.020279525006144286]\nIntercept: -1.3677594308099525\nRoot Mean Squared Error (RMSE) on test data: 0.07615891582124568\nMean Absolute Error (MAE) on test data: 0.05460223519961452\nMean Squared Error (MSE) on test data: 0.0058001804590675846\nMean Absolute Percentage Error (MAPE) on test data: 8.858109656537529%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"Chance_of_Admit\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "\n",
    "# Evaluate the model using MAE\n",
    "mae_evaluator = RegressionEvaluator(labelCol=\"Chance_of_Admit\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = mae_evaluator.evaluate(predictions)\n",
    "\n",
    "# Evaluate the model using MSE\n",
    "mse_evaluator = RegressionEvaluator(labelCol=\"Chance_of_Admit\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = mse_evaluator.evaluate(predictions)\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "predictions = predictions.withColumn(\"absolute_error\", abs(col(\"prediction\") - col(\"Chance_of_Admit\")))\n",
    "predictions = predictions.withColumn(\"percentage_error\", col(\"absolute_error\") / col(\"Chance_of_Admit\"))\n",
    "mape = predictions.selectExpr(\"mean(percentage_error) as MAPE\").collect()[0][\"MAPE\"] * 100\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE) on test data: {mae}\")\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) on test data: {mape}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bd003f-0b33-4a4e-9e0b-ea5c1df118c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------------+\n|        prediction|Chance_of_Admit|            features|\n+------------------+---------------+--------------------+\n|0.5690680965373631|           0.64|[293.0,97.0,2.0,2...|\n|0.4575038118777415|           0.47|[295.0,96.0,2.0,1...|\n|0.5252641716207944|           0.69|[295.0,101.0,2.0,...|\n| 0.517547871552895|            0.6|[296.0,101.0,1.0,...|\n| 0.519501855786225|           0.44|[298.0,98.0,2.0,1...|\n+------------------+---------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show some sample predictions\n",
    "predictions.select(\"prediction\", \"Chance_of_Admit\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85577bd4-0efb-499b-8c21-3b28b2d5e81b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----+\n|Chance_of_Admit|         prediction|count|\n+---------------+-------------------+-----+\n|           0.84| 0.8055232646273054|    1|\n|           0.84| 0.8324665476939876|    1|\n|           0.89| 0.8556019589233999|    1|\n|           0.84| 0.7300319411626703|    1|\n|           0.42| 0.5780706595995149|    1|\n|           0.68| 0.6427334663227757|    1|\n|           0.92| 0.8923168272132993|    1|\n|           0.69| 0.6345901719948519|    1|\n|           0.63|  0.579325347266817|    1|\n|           0.94| 0.9523583592136564|    1|\n|           0.95| 0.9397956184170275|    1|\n|           0.54| 0.5394947255114388|    1|\n|            0.6|  0.517547871552895|    1|\n|           0.52| 0.7556021156482222|    1|\n|           0.67| 0.5799590930399572|    1|\n|           0.36|0.41230903474397484|    1|\n|           0.91| 0.8577255286149286|    1|\n|           0.74| 0.8073395046877359|    1|\n|            0.7| 0.7779591460733304|    1|\n|           0.75| 0.7484410646155211|    1|\n+---------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate Confusion Matrix:\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "predictions.groupBy(\"Chance_of_Admit\", \"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c360dd-c95f-4462-8109-0b3c00ec25da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the trained linear regression model\n",
    "model_path = \"./Internship_Sem-6_models/Chance_of_addmission_model\"\n",
    "lr_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79265708-15fe-4a89-b9a9-e8c4bdaad701",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/Internship_Sem-6_models/Chance_of_addmission_model/data/', name='data/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/Internship_Sem-6_models/Chance_of_addmission_model/metadata/', name='metadata/', size=0, modificationTime=0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"dbfs:/Internship_Sem-6_models/Chance_of_addmission_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939b7591-1638-4468-957a-741cb136eb64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2edac5-da3b-4103-93e8-d7db6cedd940",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"dbfs:/Chance_of_addmission_model\", \"file:/tmp/Chance_of_addmission_model\",recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00663fe-1081-4183-a370-f6f2b2e72693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls /tmp/Chance_of_addmission_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296702f2-43d5-4073-9bcd-7e45885a7259",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "zip -r /tmp/Chance_of_addmission_model.zip /tmp/Chance_of_addmission_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449192f6-9351-4cab-a57a-3b90f08660f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/tmp/Chance_of_addmission_model.zip\",\"dbfs:/FileStore/Chance_of_addmission_model.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3496b15-d63f-4ed5-8e49-68cd821b5a04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/FileStore\")\n",
    "#dbutils.fs.rm(\"dbfs:/Chance_of_addmission_model/\",recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1479952110180802,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Chance_Of_Addmission_Prediction",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
